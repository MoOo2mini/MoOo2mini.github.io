<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>The Effect of Batch Size in LoRA Training</title>
  <meta name="description" content="Exploring how batch size impacts LoRA-based fine-tuning for LLMs." />
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      max-width: 900px;
      margin: 2rem auto;
      padding: 0 1rem;
      color: #333;
      line-height: 1.6;
      display: block;
    }

    header.page-header {
      margin-bottom: 2rem;
    }

    header.page-header > h1 {
      font-size: 2.5rem;
      margin-bottom: 0.3rem;
    }

    header.page-header > p {
      margin-top: 0;
      color: #555;
      font-style: italic;
    }

    header.page-header p.authors,
    header.page-header p.date {
      font-weight: 600;
      margin: 0.1rem 0;
    }

    .layout {
      display: flex;
    }

    nav.toc {
      flex: 0 0 220px;
      position: sticky;
      top: 2rem;
      padding-right: 1rem;
      border-right: 1px solid #ddd;
      height: calc(100vh - 4rem);
      overflow-y: auto;
    }

    nav.toc h3 {
      font-weight: 700;
      margin-bottom: 1rem;
    }

    nav.toc ul {
      list-style: none;
      padding-left: 0;
      margin: 0;
    }

    nav.toc li {
      margin-bottom: 0.5rem;
    }

    nav.toc a {
      text-decoration: none;
      color: #111;
      font-weight: 600;
      cursor: pointer;
    }

    nav.toc a:hover {
      text-decoration: underline;
      color: #0077cc;
    }

    main.content {
      flex: 1 1 auto;
      padding-left: 2rem;
      max-width: 100%;
    }

    section {
      margin-top: 3rem;
    }

    section > h2 {
      font-size: 1.8rem;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3rem;
      margin-bottom: 1rem;
    }

    figure {
      text-align: center;
      margin: 2rem 0;
    }

    figcaption {
      font-size: 0.9rem;
      color: #666;
      margin-top: 0.3rem;
    }

    code {
      background-color: #f4f4f4;
      padding: 0.2rem 0.4rem;
      border-radius: 3px;
      font-family: monospace;
    }

    pre code {
      display: block;
      padding: 1rem;
      overflow-x: auto;
    }

    sup a {
      text-decoration: none;
      color: #777;
      font-size: 0.7rem;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 2rem 0;
    }

    .author-metadata {
      margin: 2rem 0;
    }
    
    .author-grid {
      display: flex;
      justify-content: space-between;
      gap: 2rem;
      font-size: 1rem;
      line-height: 1.6;
    }
    
    .author-grid > div {
      flex: 1;
    }
  </style>
</head>
<body>

  <!-- Title and Author Info -->
  <header class="page-header">
    <h1>The Effect of Batch Size in LoRA Training</h1>
    <p>Exploring how batch size impacts LoRA-based fine-tuning for LLMs.</p>
    <hr>
    <section class="author-metadata">
      <div class="author-grid">
        <div>
          <strong>AUTHORS</strong><br/>
          Sangyoon Lee<br/>
          Minhee Lee<br/>
          Jiyun Bae
        </div>
        <div>
          <strong>AFFILIATIONS</strong><br/>
          Postech GSAI<br/>
          Postech GSAI<br/>
          Postech GSAI
        </div>
        <div>
          <strong>PUBLISHED</strong><br/>
          May 29, 2025
        </div>
      </div>
    </section>
    </header>
  <hr/>
  <!-- Flex Container: TOC + Content -->
  <div class="layout">
    <nav class="toc" aria-label="Table of Contents">
      <h3>Contents</h3>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#background">Background</a></li>
        <li><a href="#the-role-of-batch-size-in-lora-training">The Role of Batch Size in LoRA Training</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </nav>

    <main class="content">

<h2 id="introduction">Introduction</h2>

<p><strong>Low-Rank Adaptation (LoRA)</strong> [1] has emerged as a widely adopted technique for efficiently fine-tuning large language models (LLMs). By injecting lightweight, trainable low-rank matrices into pretrained weights, LoRA offers a practical solution for adapting massive models without the full cost of end-to-end training. Despite its growing popularity, its sensitivity to training hyperparameters, especially <strong>batch size</strong>, remains underexplored. This presents a challenge in real-world scenarios, where LoRA is often used in resource-constrained environments that demand quick, reliable hyperparameter choices without exhaustive tuning.</p>

<p>Complicating matters further, recent LoRA variants such as PiSSA [2] and MiLoRA [3] propose seemingly contradictory initialization strategies (principal vs. minor singular components), yet each work reports gain based on different experimental setups. This lack of consistency makes it difficult to discern whether observed improvements stem from algorithmic advances or simply from favorable training configurations. As a result, best practices remain unclear, and the actual influence of design choices like initialization and batch size is not directly observable.</p>

<p>In this post, we explore <strong>how batch size influences the training of LoRA-based methods</strong>.</p>

<blockquote>
  <p><strong>Our main contributions are as follows:</strong></p>
  <ul>
    1. We show that batch size plays a critical role in LoRA fine-tuning, with up to X% variation in test accuracy depending on its setting.
    2. We demonstrate that vanilla LoRA can match or even outperform recent variants, simply by tuning the batch size appropriately.
    3. We uncover non-monotonic trends in LoRA’s performance as batch size increases, underscoring the need for a deeper understanding of its optimization behavior.
  </ul>
</blockquote>
      
<h2 id="introduction">Introduction</h2>

<figure>
  <iframe src="asset/humaneval_1epoch_motif.pdf" width="100%" height="600px" style="border: none;"></iframe>
  <figcaption style="text-align: center; font-style: italic; margin-top: 8px;">
    Figure 1. Test accuracy of PiSSA and MiLoRA on HumanEval after 1 epoch of training on CodeFeedback.
  </figcaption>
</figure>

<p>
  To check the difference in performance by parameter, we fine-tune
  <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf">Llama-2-7B</a>
  using LoRA’s recent variants, PiSSA and MiLoRA, following the hyperparameter configuration proposed by Meng et al. [2].
  As shown in Figure 1, we evaluate their test accuracy on the HumanEval benchmark after a single epoch of training on the CodeFeedback dataset.
  While the original configuration favors PiSSA, we find that MiLoRA outperforms by simply reducing the batch size, without any modification.
</p>

<p>
  This result highlights a key insight: training effectiveness in LoRA-based methods is highly sensitive to configuration choices, particularly batch size.
  Yet, these factors are often overlooked in comparative studies.
</p>

<p>
  To demystify the impact of batch size, we focus on the following two key questions:
</p>

<p>
  1. How does batch size affect the LoRA training dynamics when paired with an optimally tuned learning rate?<br>
  2. Under a fixed data budget, how can we choose a batch size that balances performance and efficiency?
</p>

<p>
  Through this lens, we aim to reveal the underappreciated role of batch size in LoRA-based adaptation
  and provide practical guidance for future studies and real-world deployments.
</p>


<h2 id="background">Background</h2>
<h3>General Effect of Batch Size</h3>
<p>In traditional mini-batch stochastic gradient descent (SGD), batch size plays a critical role in the trade-off between training speed, model generalization, and computational efficiency.</p>
<p>Smaller batches tend to provide noisier but more frequent gradient updates. This noise introduced by smaller batches acts as a form of regularization, allowing it to explore the loss landscape more robustly and avoid over-fitting to the training data. Despite each steps being based on fewer examples, frequent updates indicate that the model's parameters are adjusted more often, potentially leading to faster convergence in terms of epochs. However, increasing in update steps can result in longer total training in terms of wall-clock time.</p>
<p>Larger batches provide more stable and accurate gradients by aggregating more information per update, which typically leads to fewer steps for convergence. Each gradient update becomes closer to the true direction of descent for the data distribution, allowing the model to make more optimal progress with each step. However, large batch training comes with its own challenges. Larger batches often require proportionally higher learning rates to maintain step size. Without increasing the learning rate, large batches can result in smaller steps and slow down convergence. Moreover, very large batches, approaching full-batch gradient descent, are known to risk converging to sharper minima in the loss landscape, which generalize poorly to unseen data. This phenomenon has been observed empirically by Keskar et al. (2017), with showing significant performance drops up to a 5% lower test accuracy when using excessively large batches.</p>
<p>Therefore, finding the optimal batch size is key to balancing the trade-off between computational efficiency and generalization. This has prompted extensive research in the deep learning community, particularly regarding the scaling of batch sizes and their effects on training dynamics. A critical element of this research is the concept of Critical Batch Size (CBS), which refers to the batch size beyond which increasing the size no longer significantly reduces the number of training steps. This area of study is critical because choosing the right batch size can significantly impact both the speed of training and the final performance of the model. The ability to train large models efficiently while maintaining or improving generalization is a central challenge, and ongoing research continues to refine strategies for batch size selection and optimization.</p>
<h3>Interplay with LoRA</h3>
<p>LoRA variants 발전 + PiSSA &amp; MiLoRA 간략 설명</p>
<p>이건 GPT에서 추출 정리</p>
<p>LoRA기 때문에 다른 점 (고려사항) ex) LoRA is often used in resource-constrained environments that demand quick, reliable hyperparameter choices without exhaustive tuning.기 때문에 관련 연구가 있다거나
Fine tuning 상황</p>
<p>그리고 아직 관련 연구가 부족하다.</p>
<h2 id="the-role-of-batch-size-in-lora-training">The Role of Batch Size in LoRA Training</h2>
<h3>Experimental setup</h3>
<p>In all our experiments, we adopt the LLaMA-2-7B model as the backbone for fine-tuning. Our study focuses on comparing three popular PEFT (Parameter-Efficient Fine-Tuning) methods: LoRA, PiSSA, and MiLoRA, across a range of downstream tasks.</p>
<p>To thoroughly analyze training behavior, we vary two key hyperparameters: batch size and learning rate. Specifically, we explore batch sizes of 4, 8, 16, 64, and 128, and learning rates of 1e-3, 3e-4, 5e-5, 2e-5, 5e-6, and 1e-6.</p>
<p>These hyperparameter choices are grounded in configurations commonly used across recent PEFT literature, including CORA, LoRA+, LQ-LoRA, QLoRA, and OLoRA. By aligning with these frequently adopted settings, our goal is to ensure fair and representative comparisons that reflect real-world usage patterns in PEFT research.</p>
<p>We conduct all experiments using the Hugging Face transformers and peft libraries, and monitor training dynamics such as convergence speed and final performance under each configuration.</p>
<p>Figure X. test accuracy for same epoch</p>
<p>Figure Y. test accruacy for same steps</p>
<h2 id="conclusion">Conclusion</h2>
<h2 id="apendix?">Apendix</h2>

</main>
</body>
</html>
