<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>The Effect of Batch Size in LoRA Training</title>
  <meta name="description" content="Exploring how batch size impacts LoRA-based fine-tuning for LLMs." />
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      max-width: 900px;
      margin: 2rem auto;
      padding: 0 1rem;
      color: #333;
      line-height: 1.6;
      display: block;
    }

    header.page-header {
      margin-bottom: 2rem;
    }

    header.page-header > h1 {
      font-size: 2.5rem;
      margin-bottom: 0.3rem;
    }

    header.page-header > p {
      margin-top: 0;
      color: #555;
      font-style: italic;
    }

    header.page-header p.authors,
    header.page-header p.date {
      font-weight: 600;
      margin: 0.1rem 0;
    }

    .layout {
      display: flex;
    }

    nav.toc {
      flex: 0 0 220px;
      position: sticky;
      top: 2rem;
      padding-right: 1rem;
      border-right: 1px solid #ddd;
      height: calc(100vh - 4rem);
      overflow-y: auto;
    }

    nav.toc h3 {
      font-weight: 700;
      margin-bottom: 1rem;
    }

    nav.toc ul {
      list-style: none;
      padding-left: 0;
      margin: 0;
    }

    nav.toc li {
      margin-bottom: 0.5rem;
    }

    nav.toc a {
      text-decoration: none;
      color: #111;
      font-weight: 600;
      cursor: pointer;
    }

    nav.toc a:hover {
      text-decoration: underline;
      color: #0077cc;
    }

    main.content {
      flex: 1 1 auto;
      padding-left: 2rem;
      max-width: 100%;
    }

    section {
      margin-top: 3rem;
    }

    section > h2 {
      font-size: 1.8rem;
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3rem;
      margin-bottom: 1rem;
    }

    figure {
      text-align: center;
      margin: 2rem 0;
    }

    figcaption {
      font-size: 0.9rem;
      color: #666;
      margin-top: 0.3rem;
    }

    code {
      background-color: #f4f4f4;
      padding: 0.2rem 0.4rem;
      border-radius: 3px;
      font-family: monospace;
    }

    pre code {
      display: block;
      padding: 1rem;
      overflow-x: auto;
    }

    sup a {
      text-decoration: none;
      color: #777;
      font-size: 0.7rem;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 2rem 0;
    }

    .author-metadata {
      margin: 2rem 0;
    }
    
    .author-grid {
      display: flex;
      justify-content: space-between;
      gap: 2rem;
      font-size: 1rem;
      line-height: 1.6;
    }
    
    .author-grid > div {
      flex: 1;
    }
  </style>
</head>
<body>

  <!-- Title and Author Info -->
  <header class="page-header">
    <h1>The Effect of Batch Size in LoRA Training</h1>
    <p>Exploring how batch size impacts LoRA-based fine-tuning for LLMs.</p>
    <hr>
    <section class="author-metadata">
      <div class="author-grid">
        <div>
          <strong>AUTHORS</strong><br/>
          Sangyoon Lee<br/>
          Minhee Lee<br/>
          Jiyun Bae
        </div>
        <div>
          <strong>AFFILIATIONS</strong><br/>
          Postech GSAI<br/>
          Postech GSAI<br/>
          Postech GSAI
        </div>
        <div>
          <strong>PUBLISHED</strong><br/>
          May 29, 2025
        </div>
      </div>
    </section>
    </header>
  <hr/>
  <!-- Flex Container: TOC + Content -->
  <div class="layout">
    <nav class="toc" aria-label="Table of Contents">
      <h3>Contents</h3>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#background">Background</a></li>
        <li><a href="#the-role-of-batch-size-in-lora-training">The Role of Batch Size in LoRA Training</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
        <li><a href="#apendix?">Apendix..?</a></li>
      </ul>
    </nav>

    <main class="content">
  <header>
    <h1>The Effect of Batch Size in LoRA Training</h1>
    <p>Exploring how batch size impacts LoRA-based fine-tuning for LLMs.</p>
    <p class="authors"><strong>Author:</strong> Sangyoon Lee (Postech GSAI)</p>
    <p class="date"><strong>Published:</strong> May 29, 2025</p>
  </header>
  <hr/>

<h1>Anonymize when submitting</h1>
<p>authors:
  - name: Sangyoon Lee
    url: https://sangyoon-lee99.github.io/
    affiliations:
      name: Postech GSAI</p>
<h1>must be the exact same name as your blogpost</h1>
<hr/>
<p>bibliography: overview.bib  </p>
<h1>Add a table of contents to your post.</h1>
<h1>- make sure that TOC names match the actual section names</h1>
<h1>for hyperlinks within the post to work correctly.</h1>
<h1>- please use this format rather than manually creating a markdown table of contents.</h1>
<p>toc:
  - name: Introduction
  - name: Pre-training of Foundation Adapters
  - name: Experiments
  - name: Conclusion</p>
<h1>Below is an example of injecting additional post-specific styles.</h1>
<h1>This is used in the 'Layouts' section of this post.</h1>
<h1>If you use this post as a template, delete this _styles block.</h1>
<p>_styles: &gt;
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;</p>
<h2 id="}">  }</h2>
<h2 id="introduction">Introduction</h2>
<p>Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique for efficiently fine-tuning large language models (LLMs) by injecting lightweight trainable low-rank matrices into the model's weights. Despite its growing popularity, the optimal training configurations for LoRA, particularly the role of batch size, remain underexplored. This presents a challenge in real-world scenarios, where LoRA is often used in resource-constrained environments that demand quick, reliable hyperparameter choices without exhaustive tuning.</p>
<p>Compounding this issue, recent LoRA variants such as PiSSA and MiLoRA propose seemingly contradictory initialization strategies (principal vs. minor singular components), making it difficult to discern best practices since each work uses different experimental setups. However, these findings are based on divergent experimental setups, making it difficult to draw consistent conclusions or establish best practices. This lack of standardization contributes to performance discrepancies across studies, obscuring the true impact of design decisions.</p>
<p>In this post, we explore how batch size influences the training of LoRA-based methods, and outline two promising research directions to advance our understanding and methodology:</p>
<ol>
<li>Main Finding</li>
<li>Main Message</li>
</ol>
<h2 id="motivation">Motivation</h2>
<p>Figrue 1. 기본 PiSSA config.에서의 PiSSA, MiLoRA, LoRA results</p>
<p>We fine-tuned <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf">Llama-2-7B</a> using LoRA and its recent variants (PiSSA, MiLoRA), adopting the hyperparameter configuration from ? et al. (XXXX). Figure 1 presents the test accuracy on HumanEval benchmark after training on the Code-Feedback dataset for a single epoch. While ? et al. (XXXX) achieves strong performance under their original settings, we observe that MiLoRA outperforms other methods when simply reducing the batch size. This result highlights a key insight: training effectiveness is highly sensitive to configuration choices, especially batch size.</p>
<p>To demystify the impact of batch size on fine-tuning with low-rank adaptation, we focus on the following two questions:</p>
<ol>
<li>How does batch size affect the training dynamics of LoRA-based methods when paired with an optimally tuned learning rate?</li>
<li>Given a fixed data budget, how can we select the batch size that yields the best performance?</li>
</ol>
<p>Through this lens, we aim to clarify the relationship between batch size and fine-tuning efficacy in LoRA-style adaptation methods.</p>
<h2 id="background">Background</h2>
<h3>General Effect of Batch Size</h3>
<p>In traditional mini-batch stochastic gradient descent (SGD), batch size plays a critical role in the trade-off between training speed, model generalization, and computational efficiency.</p>
<p>Smaller batches tend to provide noisier but more frequent gradient updates. This noise introduced by smaller batches acts as a form of regularization, allowing it to explore the loss landscape more robustly and avoid over-fitting to the training data. Despite each steps being based on fewer examples, frequent updates indicate that the model's parameters are adjusted more often, potentially leading to faster convergence in terms of epochs. However, increasing in update steps can result in longer total training in terms of wall-clock time.</p>
<p>Larger batches provide more stable and accurate gradients by aggregating more information per update, which typically leads to fewer steps for convergence. Each gradient update becomes closer to the true direction of descent for the data distribution, allowing the model to make more optimal progress with each step. However, large batch training comes with its own challenges. Larger batches often require proportionally higher learning rates to maintain step size. Without increasing the learning rate, large batches can result in smaller steps and slow down convergence. Moreover, very large batches, approaching full-batch gradient descent, are known to risk converging to sharper minima in the loss landscape, which generalize poorly to unseen data. This phenomenon has been observed empirically by Keskar et al. (2017), with showing significant performance drops up to a 5% lower test accuracy when using excessively large batches.</p>
<p>Therefore, finding the optimal batch size is key to balancing the trade-off between computational efficiency and generalization. This has prompted extensive research in the deep learning community, particularly regarding the scaling of batch sizes and their effects on training dynamics. A critical element of this research is the concept of Critical Batch Size (CBS), which refers to the batch size beyond which increasing the size no longer significantly reduces the number of training steps. This area of study is critical because choosing the right batch size can significantly impact both the speed of training and the final performance of the model. The ability to train large models efficiently while maintaining or improving generalization is a central challenge, and ongoing research continues to refine strategies for batch size selection and optimization.</p>
<h3>Interplay with LoRA</h3>
<p>LoRA variants 발전 + PiSSA &amp; MiLoRA 간략 설명</p>
<p>이건 GPT에서 추출 정리</p>
<p>LoRA기 때문에 다른 점 (고려사항) ex) LoRA is often used in resource-constrained environments that demand quick, reliable hyperparameter choices without exhaustive tuning.기 때문에 관련 연구가 있다거나
Fine tuning 상황</p>
<p>그리고 아직 관련 연구가 부족하다.</p>
<h2 id="the-role-of-batch-size-in-lora-training">The Role of Batch Size in LoRA Training</h2>
<h3>Experimental setup</h3>
<p>In all our experiments, we adopt the LLaMA-2-7B model as the backbone for fine-tuning. Our study focuses on comparing three popular PEFT (Parameter-Efficient Fine-Tuning) methods: LoRA, PiSSA, and MiLoRA, across a range of downstream tasks.</p>
<p>To thoroughly analyze training behavior, we vary two key hyperparameters: batch size and learning rate. Specifically, we explore batch sizes of 4, 8, 16, 64, and 128, and learning rates of 1e-3, 3e-4, 5e-5, 2e-5, 5e-6, and 1e-6.</p>
<p>These hyperparameter choices are grounded in configurations commonly used across recent PEFT literature, including CORA, LoRA+, LQ-LoRA, QLoRA, and OLoRA. By aligning with these frequently adopted settings, our goal is to ensure fair and representative comparisons that reflect real-world usage patterns in PEFT research.</p>
<p>We conduct all experiments using the Hugging Face transformers and peft libraries, and monitor training dynamics such as convergence speed and final performance under each configuration.</p>
<p>Figure X. test accuracy for same epoch</p>
<p>Figure Y. test accruacy for same steps</p>
<h2 id="conclusion">Conclusion</h2>
<h2 id="apendix?">Apendix</h2>

</main>
</body>
</html>
